"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[7206],{2488(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-1-nervous-system/nord-replay-system","title":"NORD\'s Replay System: Simulation Data Recording and Playback","description":"Overview","source":"@site/docs/module-1-nervous-system/nord-replay-system.md","sourceDirName":"module-1-nervous-system","slug":"/module-1-nervous-system/nord-replay-system","permalink":"/ai-native-book/docs/module-1-nervous-system/nord-replay-system","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-nervous-system/nord-replay-system.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"NORD: NVIDIA Omniverse Robot Definition","permalink":"/ai-native-book/docs/module-1-nervous-system/nord-content"},"next":{"title":"Module 1 Technical Diagrams: ROS Architecture","permalink":"/ai-native-book/docs/module-1-nervous-system/technical-diagrams"}}');var i=t(4848),a=t(3023);const r={sidebar_position:4},o="NORD's Replay System: Simulation Data Recording and Playback",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Architecture and Design",id:"architecture-and-design",level:2},{value:"Replay System Components",id:"replay-system-components",level:3},{value:"Data Recording Architecture",id:"data-recording-architecture",level:2},{value:"Core Recording Engine",id:"core-recording-engine",level:3},{value:"Advanced Recording Features",id:"advanced-recording-features",level:3},{value:"Selective Recording",id:"selective-recording",level:4},{value:"Event-Based Recording",id:"event-based-recording",level:4},{value:"Data Playback System",id:"data-playback-system",level:2},{value:"Core Playback Engine",id:"core-playback-engine",level:3},{value:"Advanced Playback Features",id:"advanced-playback-features",level:3},{value:"Synchronized Multi-Robot Playback",id:"synchronized-multi-robot-playback",level:4},{value:"Analysis and Visualization Tools",id:"analysis-and-visualization-tools",level:2},{value:"Data Analysis Engine",id:"data-analysis-engine",level:3},{value:"Integration with Development Workflow",id:"integration-with-development-workflow",level:2},{value:"Validation and Testing",id:"validation-and-testing",level:3},{value:"Best Practices and Guidelines",id:"best-practices-and-guidelines",level:2},{value:"Recording Best Practices",id:"recording-best-practices",level:3},{value:"Playback Best Practices",id:"playback-best-practices",level:3},{value:"Analysis Best Practices",id:"analysis-best-practices",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"nords-replay-system-simulation-data-recording-and-playback",children:"NORD's Replay System: Simulation Data Recording and Playback"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"NORD's Replay System is a comprehensive framework for recording, storing, analyzing, and replaying robot simulation data within the NVIDIA Omniverse ecosystem. This system enables developers and researchers to capture complete simulation sessions, analyze robot behavior, debug complex scenarios, and validate robot performance against real-world data. The Replay System serves as a critical component for the iterative development and validation of physical AI systems."}),"\n",(0,i.jsx)(n.h2,{id:"architecture-and-design",children:"Architecture and Design"}),"\n",(0,i.jsx)(n.h3,{id:"replay-system-components",children:"Replay System Components"}),"\n",(0,i.jsx)(n.p,{children:"The NORD Replay System consists of several interconnected components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  NORD Replay System                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Data Logger   \u2502  \u2502  Data Manager   \u2502  \u2502  Codec  \u2502  \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502         \u2502  \u2502\n\u2502  \u2502 - Joint States  \u2502  \u2502 - Buffer Mgmt   \u2502  \u2502 - HDF5  \u2502  \u2502\n\u2502  \u2502 - Sensor Data   \u2502  \u2502 - Compression   \u2502  \u2502 - Protobuf\u2502\u2502\n\u2502  \u2502 - Env State     \u2502  \u2502 - Serialization \u2502  \u2502 - Custom\u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502   Playback      \u2502  \u2502   Analysis      \u2502              \u2502\n\u2502  \u2502   Engine        \u2502  \u2502   Tools         \u2502              \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502              \u2502\n\u2502  \u2502 - Frame Sync    \u2502  \u2502 - Visualization \u2502              \u2502\n\u2502  \u2502 - Interpolation \u2502  \u2502 - Statistics    \u2502              \u2502\n\u2502  \u2502 - Real-time     \u2502  \u2502 - Comparison    \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"data-recording-architecture",children:"Data Recording Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"core-recording-engine",children:"Core Recording Engine"}),"\n",(0,i.jsx)(n.p,{children:"The recording engine captures comprehensive robot simulation data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport h5py\nfrom datetime import datetime\nimport json\n\nclass NORDReplayEngine:\n    def __init__(self, robot_config, output_path):\n        self.robot_config = robot_config\n        self.output_path = output_path\n        self.is_recording = False\n        self.start_time = None\n        self.frame_count = 0\n\n        # Initialize recording buffers\n        self.buffers = {\n            'timestamps': [],\n            'joint_states': {\n                'positions': [],\n                'velocities': [],\n                'efforts': []\n            },\n            'sensor_data': {},\n            'environment_state': [],\n            'control_commands': [],\n            'physics_state': [],\n            'user_annotations': []\n        }\n\n        # Recording parameters\n        self.record_frequency = 60  # Hz\n        self.max_buffer_size = 10000  # frames\n        self.compression_enabled = True\n\n    def start_recording(self):\n        \"\"\"Initialize recording session\"\"\"\n        self.is_recording = True\n        self.start_time = time.time()\n        self.frame_count = 0\n\n        # Initialize sensor data buffers\n        for sensor_name in self.robot_config.get('sensors', []):\n            self.buffers['sensor_data'][sensor_name] = []\n\n        print(f\"Recording started at {datetime.now().isoformat()}\")\n        return True\n\n    def record_frame(self, robot_state, sensor_data, environment_state, control_commands):\n        \"\"\"Record a single simulation frame\"\"\"\n        if not self.is_recording:\n            return False\n\n        current_time = time.time() - self.start_time\n        self.buffers['timestamps'].append(current_time)\n\n        # Record joint states\n        joint_positions = []\n        joint_velocities = []\n        joint_efforts = []\n\n        for joint_name in self.robot_config['joints']:\n            joint_state = robot_state.get_joint_state(joint_name)\n            joint_positions.append(joint_state.position)\n            joint_velocities.append(joint_state.velocity)\n            joint_efforts.append(joint_state.effort)\n\n        self.buffers['joint_states']['positions'].append(joint_positions)\n        self.buffers['joint_states']['velocities'].append(joint_velocities)\n        self.buffers['joint_states']['efforts'].append(joint_efforts)\n\n        # Record sensor data\n        for sensor_name, data in sensor_data.items():\n            self.buffers['sensor_data'][sensor_name].append(data)\n\n        # Record environment state\n        self.buffers['environment_state'].append(environment_state)\n\n        # Record control commands\n        self.buffers['control_commands'].append(control_commands)\n\n        # Record physics state\n        physics_state = {\n            'gravity': robot_state.get_gravity(),\n            'friction': robot_state.get_contact_friction(),\n            'damping': robot_state.get_damping()\n        }\n        self.buffers['physics_state'].append(physics_state)\n\n        self.frame_count += 1\n\n        # Check buffer size and flush if needed\n        if len(self.buffers['timestamps']) >= self.max_buffer_size:\n            self.flush_buffer()\n\n        return True\n\n    def stop_recording(self):\n        \"\"\"Finalize and save recording\"\"\"\n        self.is_recording = False\n\n        # Save all buffers to file\n        self.save_recording()\n\n        print(f\"Recording stopped. Saved {self.frame_count} frames to {self.output_path}\")\n        return True\n\n    def save_recording(self):\n        \"\"\"Save recording data to file using HDF5\"\"\"\n        with h5py.File(self.output_path, 'w') as f:\n            # Save metadata\n            f.attrs['created'] = datetime.now().isoformat()\n            f.attrs['robot_config'] = json.dumps(self.robot_config)\n            f.attrs['frame_count'] = self.frame_count\n            f.attrs['duration'] = self.buffers['timestamps'][-1] if self.buffers['timestamps'] else 0\n\n            # Save timestamps\n            f.create_dataset('timestamps', data=np.array(self.buffers['timestamps']))\n\n            # Save joint states\n            joint_group = f.create_group('joint_states')\n            joint_group.create_dataset('positions',\n                                     data=np.array(self.buffers['joint_states']['positions']),\n                                     compression='gzip' if self.compression_enabled else None)\n            joint_group.create_dataset('velocities',\n                                     data=np.array(self.buffers['joint_states']['velocities']),\n                                     compression='gzip' if self.compression_enabled else None)\n            joint_group.create_dataset('efforts',\n                                     data=np.array(self.buffers['joint_states']['efforts']),\n                                     compression='gzip' if self.compression_enabled else None)\n\n            # Save sensor data\n            sensor_group = f.create_group('sensor_data')\n            for sensor_name, data_list in self.buffers['sensor_data'].items():\n                if data_list:  # Only save if there's data\n                    sensor_group.create_dataset(sensor_name,\n                                              data=np.array(data_list),\n                                              compression='gzip' if self.compression_enabled else None)\n\n            # Save other data\n            f.create_dataset('environment_state',\n                           data=np.array(self.buffers['environment_state']),\n                           compression='gzip' if self.compression_enabled else None)\n            f.create_dataset('control_commands',\n                           data=np.array(self.buffers['control_commands']),\n                           compression='gzip' if self.compression_enabled else None)\n            f.create_dataset('physics_state',\n                           data=np.array(self.buffers['physics_state']),\n                           compression='gzip' if self.compression_enabled else None)\n\n        print(f\"Recording saved to {self.output_path}\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-recording-features",children:"Advanced Recording Features"}),"\n",(0,i.jsx)(n.h4,{id:"selective-recording",children:"Selective Recording"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SelectiveReplayRecorder(NORDReplayEngine):\n    def __init__(self, robot_config, output_path):\n        super().__init__(robot_config, output_path)\n        self.recording_mask = {\n            'joint_positions': True,\n            'joint_velocities': True,\n            'joint_efforts': False,  # Only record when needed\n            'sensor_data': True,\n            'environment_state': True,\n            'control_commands': True\n        }\n\n    def record_frame(self, robot_state, sensor_data, environment_state, control_commands):\n        \"\"\"Record frame with selective data capture\"\"\"\n        if not self.is_recording:\n            return False\n\n        current_time = time.time() - self.start_time\n        self.buffers['timestamps'].append(current_time)\n\n        # Conditionally record joint states based on mask\n        if self.recording_mask['joint_positions']:\n            joint_positions = []\n            for joint_name in self.robot_config['joints']:\n                joint_state = robot_state.get_joint_state(joint_name)\n                joint_positions.append(joint_state.position)\n            self.buffers['joint_states']['positions'].append(joint_positions)\n\n        if self.recording_mask['joint_velocities']:\n            joint_velocities = []\n            for joint_name in self.robot_config['joints']:\n                joint_state = robot_state.get_joint_state(joint_name)\n                joint_velocities.append(joint_state.velocity)\n            self.buffers['joint_states']['velocities'].append(joint_velocities)\n\n        if self.recording_mask['joint_efforts']:\n            joint_efforts = []\n            for joint_name in self.robot_config['joints']:\n                joint_state = robot_state.get_joint_state(joint_name)\n                joint_efforts.append(joint_state.effort)\n            self.buffers['joint_states']['efforts'].append(joint_efforts)\n\n        # Record sensor data if enabled\n        if self.recording_mask['sensor_data']:\n            for sensor_name, data in sensor_data.items():\n                self.buffers['sensor_data'][sensor_name].append(data)\n\n        # Record other data based on mask\n        if self.recording_mask['environment_state']:\n            self.buffers['environment_state'].append(environment_state)\n\n        if self.recording_mask['control_commands']:\n            self.buffers['control_commands'].append(control_commands)\n\n        self.frame_count += 1\n        return True\n"})}),"\n",(0,i.jsx)(n.h4,{id:"event-based-recording",children:"Event-Based Recording"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class EventBasedReplayRecorder(NORDReplayEngine):\n    def __init__(self, robot_config, output_path):\n        super().__init__(robot_config, output_path)\n        self.event_triggers = []\n        self.event_buffer = []\n        self.event_recording = False\n\n    def add_event_trigger(self, condition_func, description):\n        \"\"\"Add a condition that triggers recording\"\"\"\n        self.event_triggers.append({\n            'condition': condition_func,\n            'description': description,\n            'active': False\n        })\n\n    def check_events(self, robot_state, sensor_data):\n        \"\"\"Check if any events are triggered\"\"\"\n        triggered_events = []\n\n        for i, trigger in enumerate(self.event_triggers):\n            if trigger['condition'](robot_state, sensor_data):\n                if not trigger['active']:\n                    # Event just triggered\n                    trigger['active'] = True\n                    triggered_events.append({\n                        'timestamp': time.time() - self.start_time,\n                        'description': trigger['description'],\n                        'type': 'start_recording'\n                    })\n            else:\n                if trigger['active']:\n                    # Event just ended\n                    trigger['active'] = False\n                    triggered_events.append({\n                        'timestamp': time.time() - self.start_time,\n                        'description': trigger['description'],\n                        'type': 'stop_recording'\n                    })\n\n        return triggered_events\n\n    def record_frame(self, robot_state, sensor_data, environment_state, control_commands):\n        \"\"\"Record frame with event-based logic\"\"\"\n        if not self.is_recording:\n            # Check if any events should start recording\n            events = self.check_events(robot_state, sensor_data)\n            for event in events:\n                if event['type'] == 'start_recording':\n                    self.is_recording = True\n                    print(f\"Event triggered recording: {event['description']}\")\n\n        if self.is_recording:\n            # Record the frame normally\n            super().record_frame(robot_state, sensor_data, environment_state, control_commands)\n\n            # Check if any events should stop recording\n            events = self.check_events(robot_state, sensor_data)\n            for event in events:\n                if event['type'] == 'stop_recording':\n                    self.is_recording = False\n                    print(f\"Event stopped recording: {event['description']}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"data-playback-system",children:"Data Playback System"}),"\n",(0,i.jsx)(n.h3,{id:"core-playback-engine",children:"Core Playback Engine"}),"\n",(0,i.jsx)(n.p,{children:"The playback engine reproduces recorded simulation data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class NORDPlaybackEngine:\n    def __init__(self, recording_path):\n        self.recording_path = recording_path\n        self.recording_data = None\n        self.current_frame = 0\n        self.is_playing = False\n        self.playback_speed = 1.0\n        self.loop_playback = False\n\n        # Load recording\n        self.load_recording()\n\n    def load_recording(self):\n        \"\"\"Load recording data from file\"\"\"\n        with h5py.File(self.recording_path, 'r') as f:\n            self.recording_data = {}\n\n            # Load metadata\n            self.recording_data['metadata'] = {\n                'created': f.attrs.get('created', ''),\n                'frame_count': f.attrs.get('frame_count', 0),\n                'duration': f.attrs.get('duration', 0.0)\n            }\n\n            # Load timestamps\n            self.recording_data['timestamps'] = f['timestamps'][:]\n\n            # Load joint states\n            joint_group = f['joint_states']\n            self.recording_data['joint_states'] = {\n                'positions': joint_group['positions'][:],\n                'velocities': joint_group['velocities'][:],\n                'efforts': joint_group['efforts'][:]\n            }\n\n            # Load sensor data\n            self.recording_data['sensor_data'] = {}\n            sensor_group = f['sensor_data']\n            for sensor_name in sensor_group.keys():\n                self.recording_data['sensor_data'][sensor_name] = sensor_group[sensor_name][:]\n\n            # Load other data\n            self.recording_data['environment_state'] = f['environment_state'][:]\n            self.recording_data['control_commands'] = f['control_commands'][:]\n            self.recording_data['physics_state'] = f['physics_state'][:]\n\n        print(f\"Recording loaded: {self.recording_data['metadata']['frame_count']} frames\")\n\n    def play_frame(self, target_robot, interpolate=True):\n        \"\"\"Play back a single frame of recording\"\"\"\n        if self.current_frame >= len(self.recording_data['timestamps']):\n            if self.loop_playback:\n                self.current_frame = 0  # Loop back to start\n            else:\n                return False  # End of recording\n\n        # Get current frame data\n        frame_idx = self.current_frame\n\n        # Apply joint positions\n        if frame_idx < len(self.recording_data['joint_states']['positions']):\n            joint_positions = self.recording_data['joint_states']['positions'][frame_idx]\n            joint_names = self.get_joint_names(target_robot)  # Implementation depends on robot interface\n\n            for i, joint_name in enumerate(joint_names):\n                if i < len(joint_positions):\n                    target_robot.set_joint_position(joint_name, joint_positions[i])\n\n        # Apply joint velocities if available\n        if frame_idx < len(self.recording_data['joint_states']['velocities']):\n            joint_velocities = self.recording_data['joint_states']['velocities'][frame_idx]\n            for i, joint_name in enumerate(joint_names):\n                if i < len(joint_velocities):\n                    target_robot.set_joint_velocity(joint_name, joint_velocities[i])\n\n        # Apply sensor data visualization (for debugging/analysis)\n        for sensor_name, sensor_data_list in self.recording_data['sensor_data'].items():\n            if frame_idx < len(sensor_data_list):\n                # This could trigger sensor data visualization\n                self.visualize_sensor_data(sensor_name, sensor_data_list[frame_idx])\n\n        # Apply environment state\n        if frame_idx < len(self.recording_data['environment_state']):\n            env_state = self.recording_data['environment_state'][frame_idx]\n            self.apply_environment_state(env_state)\n\n        self.current_frame += 1\n        return True\n\n    def play_sequence(self, target_robot, start_frame=0, end_frame=None, callback=None):\n        \"\"\"Play back entire sequence or specified range\"\"\"\n        self.current_frame = start_frame\n        if end_frame is None:\n            end_frame = len(self.recording_data['timestamps'])\n\n        while self.current_frame < end_frame:\n            success = self.play_frame(target_robot)\n            if not success:\n                break\n\n            if callback:\n                progress = (self.current_frame - start_frame) / (end_frame - start_frame)\n                callback(progress, self.current_frame)\n\n            # Control playback speed\n            if self.playback_speed > 0:\n                time.sleep(1.0 / (60.0 * self.playback_speed))  # Assuming 60Hz base rate\n\n    def interpolate_frame(self, frame_idx, target_robot):\n        \"\"\"Interpolate between frames for smooth playback\"\"\"\n        if frame_idx >= len(self.recording_data['timestamps']) - 1:\n            return self.play_frame(target_robot, interpolate=False)\n\n        # Get current and next frame data\n        current_pos = self.recording_data['joint_states']['positions'][frame_idx]\n        next_pos = self.recording_data['joint_states']['positions'][frame_idx + 1]\n\n        # Calculate interpolation factor (simplified)\n        current_time = self.recording_data['timestamps'][frame_idx]\n        next_time = self.recording_data['timestamps'][frame_idx + 1]\n        target_time = current_time + (next_time - current_time) * 0.5  # 50% interpolation\n\n        # Linear interpolation\n        interpolated_pos = []\n        for i in range(len(current_pos)):\n            interp_val = current_pos[i] + 0.5 * (next_pos[i] - current_pos[i])\n            interpolated_pos.append(interp_val)\n\n        # Apply interpolated positions\n        joint_names = self.get_joint_names(target_robot)\n        for i, joint_name in enumerate(joint_names):\n            if i < len(interpolated_pos):\n                target_robot.set_joint_position(joint_name, interpolated_pos[i])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-playback-features",children:"Advanced Playback Features"}),"\n",(0,i.jsx)(n.h4,{id:"synchronized-multi-robot-playback",children:"Synchronized Multi-Robot Playback"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SynchronizedPlaybackEngine:\n    def __init__(self, recording_paths):\n        self.recording_engines = []\n        self.robots = []\n\n        for path in recording_paths:\n            self.recording_engines.append(NORDPlaybackEngine(path))\n\n    def add_robot(self, robot, recording_index):\n        \"\"\"Add robot to synchronized playback\"\"\"\n        self.robots.append({\n            'robot': robot,\n            'recording_index': recording_index,\n            'offset': 0.0  # Time offset for this robot\n        })\n\n    def play_synchronized(self, start_time=0.0, duration=None):\n        \"\"\"Play multiple recordings in sync\"\"\"\n        # Find common time range\n        min_duration = min(\n            engine.recording_data['metadata']['duration']\n            for engine in self.recording_engines\n        )\n\n        if duration is None:\n            duration = min_duration\n\n        current_time = start_time\n        base_timestamp = time.time()\n\n        while current_time <= duration:\n            # Calculate target frame for each recording\n            for i, robot_info in enumerate(self.robots):\n                recording_idx = robot_info['recording_index']\n                offset = robot_info['offset']\n\n                # Find closest frame to current_time + offset\n                frame_idx = self.find_frame_at_time(\n                    self.recording_engines[recording_idx],\n                    current_time + offset\n                )\n\n                # Play that frame for the robot\n                self.recording_engines[recording_idx].current_frame = frame_idx\n                self.recording_engines[recording_idx].play_frame(robot_info['robot'])\n\n            # Update time based on real elapsed time\n            elapsed = time.time() - base_timestamp\n            current_time = start_time + elapsed * self.playback_speed\n\n    def find_frame_at_time(self, engine, target_time):\n        \"\"\"Find the frame index closest to target time\"\"\"\n        timestamps = engine.recording_data['timestamps']\n\n        # Binary search for closest frame\n        left, right = 0, len(timestamps) - 1\n        while left <= right:\n            mid = (left + right) // 2\n            if timestamps[mid] == target_time:\n                return mid\n            elif timestamps[mid] < target_time:\n                left = mid + 1\n            else:\n                right = mid - 1\n\n        # Return closest frame\n        if right < 0:\n            return 0\n        if left >= len(timestamps):\n            return len(timestamps) - 1\n\n        # Return frame with closest timestamp\n        if abs(timestamps[left] - target_time) < abs(timestamps[right] - target_time):\n            return left\n        else:\n            return right\n"})}),"\n",(0,i.jsx)(n.h2,{id:"analysis-and-visualization-tools",children:"Analysis and Visualization Tools"}),"\n",(0,i.jsx)(n.h3,{id:"data-analysis-engine",children:"Data Analysis Engine"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class NORDAnalysisEngine:\n    def __init__(self, recording_path):\n        self.playback_engine = NORDPlaybackEngine(recording_path)\n        self.recording_data = self.playback_engine.recording_data\n\n    def analyze_joint_trajectories(self, joint_name):\n        \"\"\"Analyze joint trajectory data\"\"\"\n        joint_idx = self.get_joint_index(joint_name)\n\n        positions = []\n        velocities = []\n        accelerations = []\n\n        for i in range(len(self.recording_data['joint_states']['positions'])):\n            if joint_idx < len(self.recording_data['joint_states']['positions'][i]):\n                positions.append(self.recording_data['joint_states']['positions'][i][joint_idx])\n\n        # Calculate velocities and accelerations\n        for i in range(1, len(positions)):\n            dt = (self.recording_data['timestamps'][i] -\n                  self.recording_data['timestamps'][i-1])\n            if dt > 0:\n                vel = (positions[i] - positions[i-1]) / dt\n                velocities.append(vel)\n\n        for i in range(1, len(velocities)):\n            dt = (self.recording_data['timestamps'][i+1] -\n                  self.recording_data['timestamps'][i])\n            if dt > 0:\n                acc = (velocities[i] - velocities[i-1]) / dt\n                accelerations.append(acc)\n\n        return {\n            'positions': positions,\n            'velocities': velocities,\n            'accelerations': accelerations,\n            'timestamps': self.recording_data['timestamps'][:len(positions)]\n        }\n\n    def calculate_performance_metrics(self):\n        \"\"\"Calculate key performance metrics\"\"\"\n        metrics = {}\n\n        # Calculate tracking accuracy if reference trajectory exists\n        if 'reference_trajectory' in self.recording_data:\n            tracking_errors = []\n            for i in range(min(len(self.recording_data['joint_states']['positions']),\n                              len(self.recording_data['reference_trajectory']))):\n                actual = self.recording_data['joint_states']['positions'][i]\n                reference = self.recording_data['reference_trajectory'][i]\n\n                error = np.linalg.norm(np.array(actual) - np.array(reference))\n                tracking_errors.append(error)\n\n            metrics['tracking_accuracy'] = {\n                'mean_error': np.mean(tracking_errors),\n                'max_error': np.max(tracking_errors),\n                'rmse': np.sqrt(np.mean(np.array(tracking_errors) ** 2))\n            }\n\n        # Calculate execution time statistics\n        timestamps = self.recording_data['timestamps']\n        metrics['execution_time'] = {\n            'total_duration': timestamps[-1] - timestamps[0] if timestamps else 0,\n            'average_step_time': np.mean(np.diff(timestamps)) if len(timestamps) > 1 else 0,\n            'min_step_time': np.min(np.diff(timestamps)) if len(timestamps) > 1 else 0,\n            'max_step_time': np.max(np.diff(timestamps)) if len(timestamps) > 1 else 0\n        }\n\n        # Calculate energy consumption (estimated from efforts)\n        total_energy = 0\n        for i in range(len(self.recording_data['joint_states']['efforts']) - 1):\n            efforts = self.recording_data['joint_states']['efforts'][i]\n            dt = (self.recording_data['timestamps'][i+1] -\n                  self.recording_data['timestamps'][i])\n            power = sum(abs(effort) * 1.0 for effort in efforts)  # Simplified power calculation\n            total_energy += power * dt\n\n        metrics['energy_consumption'] = total_energy\n\n        return metrics\n\n    def compare_recordings(self, other_recording_path):\n        \"\"\"Compare two recordings\"\"\"\n        other_engine = NORDAnalysisEngine(other_recording_path)\n\n        comparison = {\n            'duration_difference': (\n                self.recording_data['metadata']['duration'] -\n                other_engine.recording_data['metadata']['duration']\n            ),\n            'frame_count_difference': (\n                self.recording_data['metadata']['frame_count'] -\n                other_engine.recording_data['metadata']['frame_count']\n            )\n        }\n\n        # Compare joint trajectories\n        comparison['joint_trajectory_similarity'] = {}\n        for joint_idx in range(min(\n            len(self.recording_data['joint_states']['positions'][0]),\n            len(other_engine.recording_data['joint_states']['positions'][0])\n        )):\n            self_joint_data = [frame[joint_idx] for frame in\n                              self.recording_data['joint_states']['positions']]\n            other_joint_data = [frame[joint_idx] for frame in\n                               other_engine.recording_data['joint_states']['positions']]\n\n            # Pad shorter sequence\n            min_len = min(len(self_joint_data), len(other_joint_data))\n            similarity = np.corrcoef(self_joint_data[:min_len],\n                                   other_joint_data[:min_len])[0, 1]\n\n            comparison['joint_trajectory_similarity'][f'joint_{joint_idx}'] = similarity\n\n        return comparison\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-development-workflow",children:"Integration with Development Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class NORDReplayValidator:\n    def __init__(self, robot_config):\n        self.robot_config = robot_config\n\n    def validate_recording_integrity(self, recording_path):\n        \"\"\"Validate that recording data is complete and consistent\"\"\"\n        try:\n            engine = NORDPlaybackEngine(recording_path)\n            data = engine.recording_data\n\n            validation_results = {\n                'timestamps_valid': self._validate_timestamps(data['timestamps']),\n                'joint_data_consistency': self._validate_joint_data_consistency(data['joint_states']),\n                'sensor_data_completeness': self._validate_sensor_data_completeness(\n                    data['sensor_data'], data['timestamps']\n                ),\n                'metadata_consistency': self._validate_metadata_consistency(data, recording_path)\n            }\n\n            return all(validation_results.values()), validation_results\n\n        except Exception as e:\n            return False, {'error': str(e)}\n\n    def _validate_timestamps(self, timestamps):\n        \"\"\"Validate timestamp sequence\"\"\"\n        if len(timestamps) < 2:\n            return len(timestamps) == 1  # Single frame is valid\n\n        # Check that timestamps are monotonically increasing\n        return all(timestamps[i] <= timestamps[i+1] for i in range(len(timestamps)-1))\n\n    def _validate_joint_data_consistency(self, joint_states):\n        \"\"\"Validate that joint state arrays have consistent dimensions\"\"\"\n        pos_len = len(joint_states['positions'])\n        vel_len = len(joint_states['velocities'])\n        eff_len = len(joint_states['efforts'])\n\n        # All should have same number of frames\n        return pos_len == vel_len == eff_len\n\n    def regression_test(self, test_recording_path, baseline_recording_path, threshold=0.01):\n        \"\"\"Perform regression testing by comparing recordings\"\"\"\n        test_analyzer = NORDAnalysisEngine(test_recording_path)\n        baseline_analyzer = NORDAnalysisEngine(baseline_recording_path)\n\n        # Compare key metrics\n        test_metrics = test_analyzer.calculate_performance_metrics()\n        baseline_metrics = baseline_analyzer.calculate_performance_metrics()\n\n        regression_results = {}\n\n        # Compare execution time\n        time_diff = abs(\n            test_metrics['execution_time']['total_duration'] -\n            baseline_metrics['execution_time']['total_duration']\n        )\n        regression_results['execution_time_regression'] = time_diff < threshold\n\n        # Compare energy consumption\n        if 'energy_consumption' in test_metrics and 'energy_consumption' in baseline_metrics:\n            energy_diff = abs(\n                test_metrics['energy_consumption'] -\n                baseline_metrics['energy_consumption']\n            ) / baseline_metrics['energy_consumption']\n            regression_results['energy_regression'] = energy_diff < threshold\n\n        # Compare tracking accuracy if available\n        if ('tracking_accuracy' in test_metrics and\n            'tracking_accuracy' in baseline_metrics):\n            accuracy_diff = abs(\n                test_metrics['tracking_accuracy']['rmse'] -\n                baseline_metrics['tracking_accuracy']['rmse']\n            )\n            regression_results['tracking_accuracy_regression'] = accuracy_diff < threshold\n\n        overall_pass = all(regression_results.values())\n\n        return {\n            'pass': overall_pass,\n            'results': regression_results,\n            'details': {\n                'test_metrics': test_metrics,\n                'baseline_metrics': baseline_metrics\n            }\n        }\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-and-guidelines",children:"Best Practices and Guidelines"}),"\n",(0,i.jsx)(n.h3,{id:"recording-best-practices",children:"Recording Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate recording frequency based on robot dynamics"}),"\n",(0,i.jsx)(n.li,{children:"Include sufficient pre- and post-event data for analysis"}),"\n",(0,i.jsx)(n.li,{children:"Record both low-level joint data and high-level commands"}),"\n",(0,i.jsx)(n.li,{children:"Include environmental context in recordings"}),"\n",(0,i.jsx)(n.li,{children:"Use descriptive file naming conventions"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"playback-best-practices",children:"Playback Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Validate robot state before starting playback"}),"\n",(0,i.jsx)(n.li,{children:"Monitor for interpolation errors in real-time playback"}),"\n",(0,i.jsx)(n.li,{children:"Implement safety checks during playback"}),"\n",(0,i.jsx)(n.li,{children:"Use appropriate playback speeds for analysis"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"analysis-best-practices",children:"Analysis Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Establish baseline performance metrics"}),"\n",(0,i.jsx)(n.li,{children:"Use statistical methods for comparison"}),"\n",(0,i.jsx)(n.li,{children:"Document analysis methodology"}),"\n",(0,i.jsx)(n.li,{children:"Include uncertainty estimates in results"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The NORD Replay System provides a comprehensive solution for recording, analyzing, and replaying robot simulation data, enabling rigorous validation and iterative improvement of physical AI systems."})]})}function _(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},3023(e,n,t){t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);