"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[3461],{3023(e,n,t){t.d(n,{R:()=>r,x:()=>s});var a=t(6540);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}},4424(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>_,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-3-ai-brain/practical-examples","title":"Practical Examples: AI Robot Brain Implementation","description":"Overview","source":"@site/docs/module-3-ai-brain/practical-examples.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/practical-examples","permalink":"/ai-native-book/docs/module-3-ai-brain/practical-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/practical-examples.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Assessment: AI Robot Brain Concepts","permalink":"/ai-native-book/docs/module-3-ai-brain/module-3-assessment"},"next":{"title":"Connections Between Modules 1, 2, and 3","permalink":"/ai-native-book/docs/module-3-ai-brain/module-1-2-3-connections"}}');var o=t(4848),i=t(3023);const r={sidebar_position:9},s="Practical Examples: AI Robot Brain Implementation",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Example 1: Autonomous Navigation with Obstacle Avoidance",id:"example-1-autonomous-navigation-with-obstacle-avoidance",level:2},{value:"Problem Statement",id:"problem-statement",level:3},{value:"Implementation Approach",id:"implementation-approach",level:3},{value:"Configuration Files",id:"configuration-files",level:3},{value:"Example 2: Object Recognition and Manipulation",id:"example-2-object-recognition-and-manipulation",level:2},{value:"Problem Statement",id:"problem-statement-1",level:3},{value:"Implementation Approach",id:"implementation-approach-1",level:3},{value:"Example 3: Behavior Tree for Complex Task Execution",id:"example-3-behavior-tree-for-complex-task-execution",level:2},{value:"Problem Statement",id:"problem-statement-2",level:3},{value:"Behavior Tree XML",id:"behavior-tree-xml",level:3},{value:"Custom Behavior Tree Node",id:"custom-behavior-tree-node",level:3},{value:"Example 4: Hardware Abstraction with ros_control",id:"example-4-hardware-abstraction-with-ros_control",level:2},{value:"Problem Statement",id:"problem-statement-3",level:3},{value:"Hardware Interface Implementation",id:"hardware-interface-implementation",level:3},{value:"Example 5: Integration with Previous Modules",id:"example-5-integration-with-previous-modules",level:2},{value:"Connecting to Digital Twin Environment",id:"connecting-to-digital-twin-environment",level:3},{value:"Launch File Example",id:"launch-file-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"practical-examples-ai-robot-brain-implementation",children:"Practical Examples: AI Robot Brain Implementation"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This section provides practical examples and implementation guides for the AI Robot Brain concepts covered in this module. Each example demonstrates real-world applications of navigation, perception, control, and planning systems using ROS 2."}),"\n",(0,o.jsx)(n.h2,{id:"example-1-autonomous-navigation-with-obstacle-avoidance",children:"Example 1: Autonomous Navigation with Obstacle Avoidance"}),"\n",(0,o.jsx)(n.h3,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,o.jsx)(n.p,{children:"Implement an autonomous navigation system for a mobile robot that can navigate to goals while avoiding both static and dynamic obstacles."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-approach",children:"Implementation Approach"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'// Navigation node implementation\n#include <rclcpp/rclcpp.hpp>\n#include <nav2_msgs/action/navigate_to_pose.hpp>\n#include <rclcpp_action/rclcpp_action.hpp>\n\nclass AutonomousNavigator : public rclcpp::Node\n{\npublic:\n    AutonomousNavigator() : Node("autonomous_navigator")\n    {\n        // Create action client for navigation\n        navigate_action_client_ = rclcpp_action::create_client<nav2_msgs::action::NavigateToPose>(\n            this->get_node_base_interface(),\n            this->get_node_graph_interface(),\n            this->get_node_logging_interface(),\n            this->get_node_waitables_interface(),\n            "navigate_to_pose");\n    }\n\n    void navigateToGoal(double x, double y, double theta)\n    {\n        // Check if action server is available\n        if (!navigate_action_client_->wait_for_action_server(std::chrono::seconds(5))) {\n            RCLCPP_ERROR(this->get_logger(), "Navigation action server not available");\n            return;\n        }\n\n        // Create navigation goal\n        auto goal_msg = nav2_msgs::action::NavigateToPose::Goal();\n        goal_msg.pose.header.frame_id = "map";\n        goal_msg.pose.pose.position.x = x;\n        goal_msg.pose.pose.position.y = y;\n        goal_msg.pose.pose.position.z = 0.0;\n\n        // Set orientation (theta in radians)\n        tf2::Quaternion q;\n        q.setRPY(0, 0, theta);\n        goal_msg.pose.pose.orientation.x = q.x();\n        goal_msg.pose.pose.orientation.y = q.y();\n        goal_msg.pose.pose.orientation.z = q.z();\n        goal_msg.pose.pose.orientation.w = q.w();\n\n        // Send navigation goal\n        auto send_goal_options = rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SendGoalOptions();\n        send_goal_options.result_callback = [this](const auto & result) {\n            if (result.code == rclcpp_action::ResultCode::SUCCEEDED) {\n                RCLCPP_INFO(this->get_logger(), "Navigation succeeded!");\n            } else {\n                RCLCPP_ERROR(this->get_logger(), "Navigation failed!");\n            }\n        };\n\n        navigate_action_client_->async_send_goal(goal_msg, send_goal_options);\n    }\n\nprivate:\n    rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SharedPtr navigate_action_client_;\n};\n'})}),"\n",(0,o.jsx)(n.h3,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,o.jsxs)(n.p,{children:["Navigation configuration (",(0,o.jsx)(n.code,{children:"nav2_params.yaml"}),"):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'amcl:\n  ros__parameters:\n    use_sim_time: False\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_footprint"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    likelihood_max_dist: 2.0\n    set_initial_pose: true\n    initial_pose:\n      x: 0.0\n      y: 0.0\n      z: 0.0\n      yaw: 0.0\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: False\n    global_frame: map\n    robot_base_frame: base_footprint\n    odom_topic: /odom\n    default_bt_xml_filename: "navigate_w_replanning_and_recovery.xml"\n    plugin_lib_names:\n      - nav2_compute_path_to_pose_action_bt_node\n      - nav2_follow_path_action_bt_node\n      - nav2_back_up_action_bt_node\n      - nav2_spin_action_bt_node\n      - nav2_wait_action_bt_node\n      - nav2_clear_costmap_service_bt_node\n      - nav2_is_stuck_condition_bt_node\n      - nav2_goal_reached_condition_bt_node\n      - nav2_goal_updated_condition_bt_node\n      - nav2_initial_pose_received_condition_bt_node\n      - nav2_reinitialize_global_localization_service_bt_node\n      - nav2_rate_controller_bt_node\n      - nav2_distance_controller_bt_node\n      - nav2_speed_controller_bt_node\n      - nav2_truncate_path_action_bt_node\n      - nav2_goal_updater_node_bt_node\n      - nav2_recovery_node_bt_node\n      - nav2_pipeline_sequence_bt_node\n      - nav2_round_robin_node_bt_node\n      - nav2_transform_available_condition_bt_node\n      - nav2_time_expired_condition_bt_node\n      - nav2_path_expiring_timer_condition\n      - nav2_distance_traveled_condition_bt_node\n      - nav2_single_trigger_bt_node\n      - nav2_is_battery_low_condition_bt_node\n      - nav2_navigate_through_poses_action_bt_node\n      - nav2_navigate_to_pose_action_bt_node\n      - nav2_remove_passed_goals_action_bt_node\n      - nav2_planner_selector_bt_node\n      - nav2_controller_selector_bt_node\n      - nav2_goal_checker_selector_bt_node\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-2-object-recognition-and-manipulation",children:"Example 2: Object Recognition and Manipulation"}),"\n",(0,o.jsx)(n.h3,{id:"problem-statement-1",children:"Problem Statement"}),"\n",(0,o.jsx)(n.p,{children:"Create a system that recognizes objects using computer vision and plans manipulation actions to pick and place objects."}),"\n",(0,o.jsx)(n.h3,{id:"implementation-approach-1",children:"Implementation Approach"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'// Object recognition and manipulation node\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n#include <moveit/move_group_interface/move_group_interface.h>\n#include <geometry_msgs/msg/pose.hpp>\n\nclass ObjectManipulator : public rclcpp::Node\n{\npublic:\n    ObjectManipulator() : Node("object_manipulator")\n    {\n        // Initialize MoveIt interface\n        move_group_interface_ = std::make_shared<moveit::planning_interface::MoveGroupInterface>(\n            shared_from_this(), "manipulator");\n\n        // Create image subscription\n        image_subscription_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "/camera/rgb/image_raw", 10,\n            std::bind(&ObjectManipulator::imageCallback, this, std::placeholders::_1));\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Perform object detection (simplified example)\n        cv::Mat hsv, mask;\n        cv::cvtColor(cv_ptr->image, hsv, cv::COLOR_BGR2HSV);\n\n        // Detect red objects (example)\n        cv::Scalar lower_red(0, 50, 50);\n        cv::Scalar upper_red(10, 255, 255);\n        cv::inRange(hsv, lower_red, upper_red, mask);\n\n        // Find contours\n        std::vector<std::vector<cv::Point>> contours;\n        cv::findContours(mask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);\n\n        if (!contours.empty()) {\n            // Find largest contour (assuming it\'s our target object)\n            auto largest_contour = std::max_element(contours.begin(), contours.end(),\n                [](const std::vector<cv::Point>& a, const std::vector<cv::Point>& b) {\n                    return cv::contourArea(a) < cv::contourArea(b);\n                });\n\n            // Calculate centroid\n            cv::Moments moments = cv::moments(*largest_contour);\n            if (moments.m00 != 0) {\n                int cx = moments.m10 / moments.m00;\n                int cy = moments.m01 / moments.m00;\n\n                RCLCPP_INFO(this->get_logger(), "Object detected at (%d, %d)", cx, cy);\n\n                // Plan manipulation to reach the object\n                planToReachObject(cx, cy, cv_ptr->image.cols, cv_ptr->image.rows);\n            }\n        }\n    }\n\n    void planToReachObject(int img_x, int img_y, int img_width, int img_height)\n    {\n        // Convert image coordinates to world coordinates (simplified)\n        // In practice, you would use camera calibration and depth information\n        geometry_msgs::msg::Pose target_pose;\n        target_pose.position.x = (img_x - img_width/2) * 0.001; // Scale factor\n        target_pose.position.y = (img_height/2 - img_y) * 0.001; // Scale factor\n        target_pose.position.z = 0.1; // Height above table\n\n        // Set orientation\n        target_pose.orientation.w = 1.0;\n        target_pose.orientation.x = 0.0;\n        target_pose.orientation.y = 0.0;\n        target_pose.orientation.z = 0.0;\n\n        // Plan and execute motion\n        move_group_interface_->setPoseTarget(target_pose);\n\n        moveit::planning_interface::MoveGroupInterface::Plan plan;\n        bool success = (move_group_interface_->plan(plan) == moveit::core::MoveItErrorCode::SUCCESS);\n\n        if (success) {\n            RCLCPP_INFO(this->get_logger(), "Motion plan successful, executing...");\n            move_group_interface_->execute(plan);\n        } else {\n            RCLCPP_ERROR(this->get_logger(), "Motion planning failed!");\n        }\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_subscription_;\n    std::shared_ptr<moveit::planning_interface::MoveGroupInterface> move_group_interface_;\n};\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-3-behavior-tree-for-complex-task-execution",children:"Example 3: Behavior Tree for Complex Task Execution"}),"\n",(0,o.jsx)(n.h3,{id:"problem-statement-2",children:"Problem Statement"}),"\n",(0,o.jsx)(n.p,{children:"Implement a behavior tree for a robot that must patrol an area, detect anomalies, and respond appropriately."}),"\n",(0,o.jsx)(n.h3,{id:"behavior-tree-xml",children:"Behavior Tree XML"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <ReactiveSequence>\n            \x3c!-- Check if emergency stop is active --\x3e\n            <Inverter>\n                <CheckEmergencyStop/>\n            </Inverter>\n\n            \x3c!-- Main patrol behavior --\x3e\n            <Sequence>\n                <GetNextPatrolWaypoint waypoint="{waypoint}"/>\n                <NavigateToPose goal="{waypoint}"/>\n\n                \x3c!-- Monitor during patrol --\x3e\n                <ReactiveFallback>\n                    \x3c!-- If anomaly detected, handle it --\x3e\n                    <Sequence>\n                        <CheckAnomalyDetection result="{anomaly_detected}"/>\n                        <BoolCondition value="{anomaly_detected}"/>\n                        <HandleAnomaly anomaly="{anomaly_detected}"/>\n                    </Sequence>\n\n                    \x3c!-- Otherwise continue patrol --\x3e\n                    <Wait wait_duration="5.0"/>\n                </ReactiveFallback>\n            </Sequence>\n        </ReactiveSequence>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"custom-behavior-tree-node",children:"Custom Behavior Tree Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'// Custom behavior tree node for anomaly detection\n#include "behaviortree_cpp_v3/behavior_tree.h"\n#include "behaviortree_cpp_v3/bt_factory.h"\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <rclcpp/rclcpp.hpp>\n\nclass CheckAnomalyDetection : public BT::SyncActionNode\n{\npublic:\n    CheckAnomalyDetection(const std::string& name, const BT::NodeConfiguration& config)\n        : BT::SyncActionNode(name, config), node_(std::make_shared<rclcpp::Node>("bt_anomaly_detector"))\n    {\n        laser_sub_ = node_->create_subscription<sensor_msgs::msg::LaserScan>(\n            "/scan", 10,\n            std::bind(&CheckAnomalyDetection::laserCallback, this, std::placeholders::_1));\n    }\n\n    BT::NodeStatus tick() override\n    {\n        // Check if we have recent laser data indicating anomaly\n        std::lock_guard<std::mutex> lock(data_mutex_);\n\n        if (has_recent_anomaly_) {\n            setOutput("result", true);\n            has_recent_anomaly_ = false; // Reset after detection\n            return BT::NodeStatus::SUCCESS;\n        }\n\n        setOutput("result", false);\n        return BT::NodeStatus::FAILURE;\n    }\n\n    static BT::PortsList providedPorts()\n    {\n        return { BT::OutputPort<bool>("result", "True if anomaly detected") };\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::msg::LaserScan::SharedPtr msg)\n    {\n        std::lock_guard<std::mutex> lock(data_mutex_);\n\n        // Simple anomaly detection: check for unexpected obstacles\n        for (size_t i = 0; i < msg->ranges.size(); ++i) {\n            if (msg->ranges[i] > 0.1 && msg->ranges[i] < 0.5) { // Obstacle within 50cm\n                has_recent_anomaly_ = true;\n                break;\n            }\n        }\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_sub_;\n    std::shared_ptr<rclcpp::Node> node_;\n    std::mutex data_mutex_;\n    bool has_recent_anomaly_ = false;\n};\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-4-hardware-abstraction-with-ros_control",children:"Example 4: Hardware Abstraction with ros_control"}),"\n",(0,o.jsx)(n.h3,{id:"problem-statement-3",children:"Problem Statement"}),"\n",(0,o.jsx)(n.p,{children:"Implement a custom hardware interface for a differential drive robot with custom motor controllers."}),"\n",(0,o.jsx)(n.h3,{id:"hardware-interface-implementation",children:"Hardware Interface Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'// Custom hardware interface for differential drive\n#include <hardware_interface/base_interface.hpp>\n#include <hardware_interface/joint_state_interface.hpp>\n#include <hardware_interface/joint_command_interface.hpp>\n#include <hardware_interface/robot_hw.hpp>\n#include <joint_limits_interface/joint_limits_interface.hpp>\n\nclass CustomDiffDriveHardwareInterface : public hardware_interface::BaseInterface<hardware_interface::JointStateInterface,\n                                                                                 hardware_interface::VelocityJointInterface>\n{\npublic:\n    bool init(ros::NodeHandle& root_nh, ros::NodeHandle& robot_hw_nh) override\n    {\n        // Get joint names from parameter server\n        std::vector<std::string> joint_names;\n        if (!robot_hw_nh.getParam("joint_names", joint_names)) {\n            ROS_ERROR("Cannot find required parameter \'joint_names\' on the parameter server.");\n            return false;\n        }\n\n        // Create joint state and command handles\n        for (size_t i = 0; i < joint_names.size(); ++i) {\n            // Initialize joint data\n            joint_position_[i] = 0.0;\n            joint_velocity_[i] = 0.0;\n            joint_effort_[i] = 0.0;\n            joint_velocity_command_[i] = 0.0;\n\n            // Connect joint state interface\n            hardware_interface::JointStateHandle joint_state_handle(\n                joint_names[i],\n                &joint_position_[i],\n                &joint_velocity_[i],\n                &joint_effort_[i]);\n            joint_state_interface_.registerHandle(joint_state_handle);\n\n            // Connect joint velocity command interface\n            hardware_interface::JointHandle joint_velocity_handle(\n                joint_state_handle,\n                &joint_velocity_command_[i]);\n            velocity_joint_interface_.registerHandle(joint_velocity_handle);\n        }\n\n        // Register interfaces\n        registerInterface(&joint_state_interface_);\n        registerInterface(&velocity_joint_interface_);\n\n        // Initialize custom hardware communication\n        if (!initializeHardwareCommunication()) {\n            ROS_ERROR("Failed to initialize hardware communication");\n            return false;\n        }\n\n        return true;\n    }\n\n    void read(const ros::Time& time, const ros::Duration& period) override\n    {\n        // Read joint states from hardware\n        for (size_t i = 0; i < joint_names_.size(); ++i) {\n            // Update position, velocity, and effort from hardware\n            joint_position_[i] = readJointPosition(i);\n            joint_velocity_[i] = readJointVelocity(i);\n            joint_effort_[i] = readJointEffort(i);\n        }\n    }\n\n    void write(const ros::Time& time, const ros::Duration& period) override\n    {\n        // Write commands to hardware\n        for (size_t i = 0; i < joint_names_.size(); ++i) {\n            writeJointVelocityCommand(i, joint_velocity_command_[i]);\n        }\n    }\n\nprivate:\n    bool initializeHardwareCommunication()\n    {\n        // Initialize communication with custom motor controllers\n        // This would contain vendor-specific initialization code\n        return true;\n    }\n\n    double readJointPosition(size_t joint_index) { /* Implementation */ return 0.0; }\n    double readJointVelocity(size_t joint_index) { /* Implementation */ return 0.0; }\n    double readJointEffort(size_t joint_index) { /* Implementation */ return 0.0; }\n    void writeJointVelocityCommand(size_t joint_index, double command) { /* Implementation */ }\n\n    hardware_interface::JointStateInterface joint_state_interface_;\n    hardware_interface::VelocityJointInterface velocity_joint_interface_;\n\n    std::vector<std::string> joint_names_;\n    std::vector<double> joint_position_;\n    std::vector<double> joint_velocity_;\n    std::vector<double> joint_effort_;\n    std::vector<double> joint_velocity_command_;\n};\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-5-integration-with-previous-modules",children:"Example 5: Integration with Previous Modules"}),"\n",(0,o.jsx)(n.h3,{id:"connecting-to-digital-twin-environment",children:"Connecting to Digital Twin Environment"}),"\n",(0,o.jsx)(n.p,{children:"The AI Robot Brain connects to the digital twin environment from Module 2 through:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Interface"}),": Using Gazebo or Unity simulation for testing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State Synchronization"}),": Keeping simulation and reality states aligned"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Validation Layer"}),": Validating behaviors in simulation before physical execution"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"launch-file-example",children:"Launch File Example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Combined launch file for AI Robot Brain --\x3e\n<launch>\n    \x3c!-- Start robot state publisher --\x3e\n    <node name="robot_state_publisher" pkg="robot_state_publisher" exec="robot_state_publisher">\n        <param name="robot_description" value="$(var robot_description)"/>\n    </node>\n\n    \x3c!-- Start controller manager --\x3e\n    <node name="controller_manager" pkg="controller_manager" exec="ros2_control_node">\n        <param name="robot_description" value="$(var robot_description)"/>\n        <remap from="/controller_manager/robot_description" to="robot_description"/>\n    </node>\n\n    \x3c!-- Load and start controllers --\x3e\n    <node name="spawner_joint_state_broadcaster" pkg="controller_manager" exec="spawner" args="joint_state_broadcaster"/>\n    <node name="spawner_velocity_controller" pkg="controller_manager" exec="spawner" args="velocity_controller"/>\n\n    \x3c!-- Start navigation system --\x3e\n    <include file="$(find-pkg-share nav2_bringup)/launch/navigation_launch.py"/>\n\n    \x3c!-- Start perception pipeline --\x3e\n    <node name="perception_pipeline" pkg="perception_pkg" exec="perception_node">\n        <param name="camera_topic" value="/camera/rgb/image_raw"/>\n        <param name="pointcloud_topic" value="/camera/depth/points"/>\n    </node>\n\n    \x3c!-- Start behavior tree executor --\x3e\n    <node name="bt_navigator" pkg="nav2_bt_navigator" exec="bt_navigator">\n        <param name="plugin_lib_names" value="[nav2_compute_path_to_pose_action_bt_node, nav2_follow_path_action_bt_node]"/>\n    </node>\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use multi-threading for perception and planning to avoid blocking control loops"}),"\n",(0,o.jsx)(n.li,{children:"Implement efficient data structures for real-time processing"}),"\n",(0,o.jsx)(n.li,{children:"Profile and optimize critical code paths"}),"\n",(0,o.jsx)(n.li,{children:"Use appropriate update rates for different system components"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement safety monitors at each level of the system"}),"\n",(0,o.jsx)(n.li,{children:"Use fail-safe behaviors when components fail"}),"\n",(0,o.jsx)(n.li,{children:"Validate all commands before execution"}),"\n",(0,o.jsx)(n.li,{children:"Implement emergency stop capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test components individually before system integration"}),"\n",(0,o.jsx)(n.li,{children:"Use simulation environments for safe testing"}),"\n",(0,o.jsx)(n.li,{children:"Implement comprehensive logging for debugging"}),"\n",(0,o.jsx)(n.li,{children:"Create unit and integration tests for all components"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"These practical examples demonstrate the implementation of AI Robot Brain concepts using ROS 2, showing how navigation, perception, control, and planning systems can be integrated to create intelligent robotic behavior. Each example provides a foundation that can be extended and customized for specific applications while maintaining the safety-first and sim-to-real principles established in previous modules."})]})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);