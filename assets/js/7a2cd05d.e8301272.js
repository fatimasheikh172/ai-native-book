"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4165],{1805(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/voice-plan-examples","title":"Voice-PLAN Interactive Examples","description":"Overview","source":"@site/docs/module-4-vla/voice-plan-examples.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-plan-examples","permalink":"/ai-native-book/docs/module-4-vla/voice-plan-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/voice-plan-examples.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 Assessment: Vision-Language-Action (VLA) Concepts","permalink":"/ai-native-book/docs/module-4-vla/module-4-assessment"},"next":{"title":"NAVIGATE and MANIPULATE Practical Demonstrations","permalink":"/ai-native-book/docs/module-4-vla/practical-demonstrations"}}');var o=i(4848),a=i(3023);const s={sidebar_position:8},l="Voice-PLAN Interactive Examples",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Example 1: Simple Navigation Command",id:"example-1-simple-navigation-command",level:2},{value:"Command: &quot;Take me to the kitchen&quot;",id:"command-take-me-to-the-kitchen",level:3},{value:"Processing Pipeline:",id:"processing-pipeline",level:4},{value:"Interactive Simulation:",id:"interactive-simulation",level:4},{value:"Code Implementation:",id:"code-implementation",level:4},{value:"Example 2: Object Manipulation Command",id:"example-2-object-manipulation-command",level:2},{value:"Command: &quot;Please pick up the red cup from the table&quot;",id:"command-please-pick-up-the-red-cup-from-the-table",level:3},{value:"Processing Pipeline:",id:"processing-pipeline-1",level:4},{value:"Interactive Simulation:",id:"interactive-simulation-1",level:4},{value:"Code Implementation:",id:"code-implementation-1",level:4},{value:"Example 3: Complex Multi-Step Command",id:"example-3-complex-multi-step-command",level:2},{value:"Command: &quot;Go to the living room and turn on the lamp next to the sofa&quot;",id:"command-go-to-the-living-room-and-turn-on-the-lamp-next-to-the-sofa",level:3},{value:"Processing Pipeline:",id:"processing-pipeline-2",level:4},{value:"Interactive Simulation:",id:"interactive-simulation-2",level:4},{value:"Code Implementation:",id:"code-implementation-2",level:4},{value:"Example 4: Conditional Command",id:"example-4-conditional-command",level:2},{value:"Command: &quot;If you see the blue book, bring it to me; otherwise, tell me what you found instead&quot;",id:"command-if-you-see-the-blue-book-bring-it-to-me-otherwise-tell-me-what-you-found-instead",level:3},{value:"Processing Pipeline:",id:"processing-pipeline-3",level:4},{value:"Interactive Simulation:",id:"interactive-simulation-3",level:4},{value:"Code Implementation:",id:"code-implementation-3",level:4},{value:"Example 5: Temporal Command",id:"example-5-temporal-command",level:2},{value:"Command: &quot;Wait for me to say &#39;go&#39;, then bring me the newspaper&quot;",id:"command-wait-for-me-to-say-go-then-bring-me-the-newspaper",level:3},{value:"Processing Pipeline:",id:"processing-pipeline-4",level:4},{value:"Interactive Simulation:",id:"interactive-simulation-4",level:4},{value:"Code Implementation:",id:"code-implementation-4",level:4},{value:"Interactive Exercise: Design Your Own Voice Command",id:"interactive-exercise-design-your-own-voice-command",level:2},{value:"Exercise Instructions:",id:"exercise-instructions",level:3},{value:"Example Template:",id:"example-template",level:3},{value:"Sample User Creation:",id:"sample-user-creation",level:3},{value:"Voice Command Best Practices",id:"voice-command-best-practices",level:2},{value:"Clear Speech Guidelines:",id:"clear-speech-guidelines",level:3},{value:"Effective Command Structure:",id:"effective-command-structure",level:3},{value:"Error Recovery:",id:"error-recovery",level:3},{value:"Advanced Voice-PLAN Features",id:"advanced-voice-plan-features",level:2},{value:"Context-Aware Processing:",id:"context-aware-processing",level:3},{value:"Multi-Language Support:",id:"multi-language-support",level:3},{value:"Speaker Recognition:",id:"speaker-recognition",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-plan-interactive-examples",children:"Voice-PLAN Interactive Examples"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This section provides interactive examples and demonstrations of the voice-PLAN capabilities in the Vision-Language-Action (VLA) system. These examples illustrate how natural language commands are processed through the Whisper integration and executed by the autonomous humanoid robot system."}),"\n",(0,o.jsx)(n.h2,{id:"example-1-simple-navigation-command",children:"Example 1: Simple Navigation Command"}),"\n",(0,o.jsx)(n.h3,{id:"command-take-me-to-the-kitchen",children:'Command: "Take me to the kitchen"'}),"\n",(0,o.jsx)(n.h4,{id:"processing-pipeline",children:"Processing Pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),': Whisper converts speech to text: "Take me to the kitchen"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language Understanding"}),": LLM-4 identifies intent as navigation request"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Location Resolution"}),': System identifies "kitchen" in the environment map']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Path Planning"}),": NAVIGATE system plans safe route to kitchen"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution"}),": Robot moves to kitchen while maintaining safety"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"interactive-simulation",children:"Interactive Simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Take me to the kitchen"\n\u2193\nWhisper: "Take me to the kitchen" [Confidence: 0.92]\n\u2193\nLLM-4: {\n  "intent": "navigation",\n  "target_location": "kitchen",\n  "action_sequence": [\n    {"type": "navigate", "destination": "kitchen"}\n  ]\n}\n\u2193\nNAVIGATE: Path planned (3.2m, 4 waypoints)\n\u2193\nRobot: Moving to kitchen... [Progress: 65%]\n\u2193\nResult: Arrived at kitchen. What would you like me to do next?\n'})}),"\n",(0,o.jsx)(n.h4,{id:"code-implementation",children:"Code Implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def handle_navigation_command(command):\n    # Step 1: Speech to text\n    transcription = whisper_transcribe(audio_input)\n\n    # Step 2: Intent recognition\n    intent_data = llm4_process_command(transcription)\n\n    # Step 3: Location resolution\n    target_location = resolve_location(intent_data['target_location'])\n\n    # Step 4: Path planning and execution\n    if intent_data['intent'] == 'navigation':\n        path = navigate_system.plan_path_to(target_location)\n        navigate_system.execute_path(path)\n        return f\"Arrived at {target_location}\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-2-object-manipulation-command",children:"Example 2: Object Manipulation Command"}),"\n",(0,o.jsx)(n.h3,{id:"command-please-pick-up-the-red-cup-from-the-table",children:'Command: "Please pick up the red cup from the table"'}),"\n",(0,o.jsx)(n.h4,{id:"processing-pipeline-1",children:"Processing Pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),': Whisper processes: "Please pick up the red cup from the table"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Parsing"}),": LLM-4 identifies manipulation intent with object specification"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Recognition"}),': Vision system identifies "red cup" in environment']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp Planning"}),": MANIPULATE system plans optimal grasp"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution"}),": Robot navigates, grasps, and picks up the object"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"interactive-simulation-1",children:"Interactive Simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Please pick up the red cup from the table"\n\u2193\nWhisper: "Please pick up the red cup from the table" [Confidence: 0.88]\n\u2193\nLLM-4: {\n  "intent": "manipulation",\n  "action": "pick_up",\n  "object": {\n    "color": "red",\n    "type": "cup",\n    "location": "table"\n  }\n}\n\u2193\nVision: Found red cup at position [1.2, 0.8, 0.75]\n\u2193\nMANIPULATE: Grasp planned (quality: 0.85)\n\u2193\nRobot: Approaching red cup... Grasping... Cup picked up successfully!\n\u2193\nResult: I\'ve picked up the red cup. Where should I place it?\n'})}),"\n",(0,o.jsx)(n.h4,{id:"code-implementation-1",children:"Code Implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def handle_manipulation_command(command):\n    # Step 1: Process command with LLM-4\n    command_data = llm4_process_command(command)\n\n    # Step 2: Identify target object\n    target_object = vision_system.find_object(\n        color=command_data['object']['color'],\n        type=command_data['object']['type'],\n        location=command_data['object']['location']\n    )\n\n    # Step 3: Plan and execute grasp\n    if target_object:\n        grasp_plan = manipulate_system.plan_grasp(target_object)\n        result = manipulate_system.execute_grasp(grasp_plan)\n        return f\"Successfully picked up the {command_data['object']['color']} {command_data['object']['type']}\"\n    else:\n        return f\"Could not find {command_data['object']['color']} {command_data['object']['type']}\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-3-complex-multi-step-command",children:"Example 3: Complex Multi-Step Command"}),"\n",(0,o.jsx)(n.h3,{id:"command-go-to-the-living-room-and-turn-on-the-lamp-next-to-the-sofa",children:'Command: "Go to the living room and turn on the lamp next to the sofa"'}),"\n",(0,o.jsx)(n.h4,{id:"processing-pipeline-2",children:"Processing Pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Whisper captures the multi-step command"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Decomposition"}),": LLM-4 breaks into navigation and interaction tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environment Analysis"}),": Vision system identifies lamp and sofa relationship"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sequential Execution"}),": NAVIGATE to location, then manipulation action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Result Verification"}),": Confirm lamp is turned on"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"interactive-simulation-2",children:"Interactive Simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Go to the living room and turn on the lamp next to the sofa"\n\u2193\nWhisper: "Go to the living room and turn on the lamp next to the sofa" [Confidence: 0.85]\n\u2193\nLLM-4: {\n  "intent": "complex_task",\n  "subtasks": [\n    {\n      "id": 1,\n      "type": "navigation",\n      "target": "living room",\n      "next": 2\n    },\n    {\n      "id": 2,\n      "type": "manipulation",\n      "action": "press_switch",\n      "object": "lamp next to sofa"\n    }\n  ]\n}\n\u2193\nNAVIGATE: Path to living room planned (5.1m)\n\u2193\nRobot: Navigating to living room...\n\u2193\nVision: Identified lamp next to sofa at [2.1, 3.2, 0.9]\n\u2193\nMANIPULATE: Approach and press switch planned\n\u2193\nRobot: Approaching lamp... Switch pressed!\n\u2193\nVision: Confirming lamp status - ON\n\u2193\nResult: I\'ve turned on the lamp next to the sofa in the living room.\n'})}),"\n",(0,o.jsx)(n.h4,{id:"code-implementation-2",children:"Code Implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def handle_complex_command(command):\n    # Step 1: Decompose complex command\n    task_plan = llm4_decompose_task(command)\n\n    # Step 2: Execute subtasks sequentially\n    for subtask in task_plan['subtasks']:\n        if subtask['type'] == 'navigation':\n            navigate_system.go_to(subtask['target'])\n        elif subtask['type'] == 'manipulation':\n            # Identify target object relative to reference\n            target = vision_system.find_object_relative_to(\n                object_type=subtask['action_object'],\n                reference_object=subtask['reference_object']\n            )\n            manipulate_system.execute_action(subtask['action'], target)\n\n    return \"Complex task completed successfully\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-4-conditional-command",children:"Example 4: Conditional Command"}),"\n",(0,o.jsx)(n.h3,{id:"command-if-you-see-the-blue-book-bring-it-to-me-otherwise-tell-me-what-you-found-instead",children:'Command: "If you see the blue book, bring it to me; otherwise, tell me what you found instead"'}),"\n",(0,o.jsx)(n.h4,{id:"processing-pipeline-3",children:"Processing Pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Whisper processes conditional logic"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Conditional Parsing"}),": LLM-4 identifies if-then-else structure"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Search"}),': Vision system actively looks for "blue book"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Conditional Execution"}),": Execute appropriate branch based on findings"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Result Communication"}),": Report outcome based on condition result"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"interactive-simulation-3",children:"Interactive Simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "If you see the blue book, bring it to me; otherwise, tell me what you found instead"\n\u2193\nWhisper: "If you see the blue book, bring it to me; otherwise, tell me what you found instead" [Confidence: 0.82]\n\u2193\nLLM-4: {\n  "intent": "conditional_task",\n  "condition": {\n    "check": "object_exists",\n    "object": {"color": "blue", "type": "book"}\n  },\n  "if_true": {\n    "action": "bring_object",\n    "target": "user"\n  },\n  "if_false": {\n    "action": "report_alternative",\n    "target": "user"\n  }\n}\n\u2193\nVision: Scanning for blue book...\n\u2193\nVision: Blue book found at [0.5, 1.8, 0.85]!\n\u2193\nMANIPULATE: Grasp planned for blue book\n\u2193\nRobot: Approaching blue book... Grasping... Bringing to user...\n\u2193\nResult: Here is the blue book you asked for.\n'})}),"\n",(0,o.jsx)(n.h4,{id:"code-implementation-3",children:"Code Implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def handle_conditional_command(command):\n    # Step 1: Parse conditional structure\n    conditional_plan = llm4_parse_conditional(command)\n\n    # Step 2: Evaluate condition\n    condition_result = evaluate_condition(conditional_plan['condition'])\n\n    # Step 3: Execute appropriate branch\n    if condition_result:\n        result = execute_action(conditional_plan['if_true'])\n    else:\n        result = execute_action(conditional_plan['if_false'])\n\n    return result\n\ndef evaluate_condition(condition):\n    if condition['check'] == 'object_exists':\n        found_object = vision_system.find_object(\n            color=condition['object']['color'],\n            type=condition['object']['type']\n        )\n        return found_object is not None\n    return False\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-5-temporal-command",children:"Example 5: Temporal Command"}),"\n",(0,o.jsx)(n.h3,{id:"command-wait-for-me-to-say-go-then-bring-me-the-newspaper",children:"Command: \"Wait for me to say 'go', then bring me the newspaper\""}),"\n",(0,o.jsx)(n.h4,{id:"processing-pipeline-4",children:"Processing Pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Whisper identifies waiting instruction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State Management"}),": System enters listening state for trigger word"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Event Detection"}),': Monitor for "go" command while maintaining readiness']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": After trigger, execute manipulation task"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Result Reporting"}),": Confirm completion"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"interactive-simulation-4",children:"Interactive Simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Wait for me to say \'go\', then bring me the newspaper"\n\u2193\nWhisper: "Wait for me to say \'go\', then bring me the newspaper" [Confidence: 0.87]\n\u2193\nLLM-4: {\n  "intent": "temporal_task",\n  "waiting_state": true,\n  "trigger": "go",\n  "action": "bring_newspaper",\n  "on_trigger": {\n    "action": "locate_and_grasp",\n    "object": "newspaper"\n  }\n}\n\u2193\nSystem: Waiting for \'go\' command... [Listening]\n\u2193\nUser: "go"\n\u2193\nWhisper: "go" [Confidence: 0.95]\n\u2193\nSystem: Trigger received! Locating newspaper...\n\u2193\nVision: Newspaper detected at [1.5, 0.2, 0.75]\n\u2193\nMANIPULATE: Grasping newspaper...\n\u2193\nRobot: Bringing newspaper to user...\n\u2193\nResult: Here is the newspaper as requested.\n'})}),"\n",(0,o.jsx)(n.h4,{id:"code-implementation-4",children:"Code Implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def handle_temporal_command(command):\n    # Parse temporal command\n    temporal_plan = llm4_parse_temporal(command)\n\n    if temporal_plan[\'waiting_state\']:\n        # Enter waiting state\n        trigger_detected = wait_for_trigger(temporal_plan[\'trigger\'])\n\n        if trigger_detected:\n            # Execute post-trigger action\n            target_object = vision_system.find_object(\n                type=temporal_plan[\'on_trigger\'][\'object\']\n            )\n            result = manipulate_system.execute_grasp(target_object)\n            return f"Action completed after trigger: {result}"\n\n    return "Temporal command processed"\n\ndef wait_for_trigger(trigger_word, timeout=30):\n    """Wait for specific trigger word with timeout"""\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        audio_input = get_audio_input()\n        transcription = whisper_transcribe(audio_input)\n        if trigger_word.lower() in transcription.lower():\n            return True\n    return False\n'})}),"\n",(0,o.jsx)(n.h2,{id:"interactive-exercise-design-your-own-voice-command",children:"Interactive Exercise: Design Your Own Voice Command"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-instructions",children:"Exercise Instructions:"}),"\n",(0,o.jsx)(n.p,{children:"Create a voice command for the VLA system that incorporates multiple elements from the examples above. Your command should include:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Type"}),": Navigation, manipulation, or both"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Specification"}),": Color, size, or other identifying features"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Conditional Logic"}),": If-then conditions (optional)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Temporal Element"}),": Timing or sequence requirements (optional)"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-template",children:"Example Template:"}),"\n",(0,o.jsx)(n.p,{children:'"Please [ACTION] the [COLOR] [OBJECT] in the [LOCATION] and [FOLLOW-UP ACTION]"'}),"\n",(0,o.jsx)(n.h3,{id:"sample-user-creation",children:"Sample User Creation:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Go to the office and if you find the black laptop on the desk, bring it to the living room; otherwise, just tell me it\'s not there."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Processing"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Navigate to office"}),"\n",(0,o.jsx)(n.li,{children:"Look for black laptop on desk"}),"\n",(0,o.jsx)(n.li,{children:"If found: bring to living room"}),"\n",(0,o.jsx)(n.li,{children:"If not found: report status"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-best-practices",children:"Voice Command Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"clear-speech-guidelines",children:"Clear Speech Guidelines:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Speak at normal volume and pace"}),"\n",(0,o.jsx)(n.li,{children:"Use clear pronunciation"}),"\n",(0,o.jsx)(n.li,{children:'State commands directly (e.g., "Go to kitchen" vs. "Could you go to the kitchen?")'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"effective-command-structure",children:"Effective Command Structure:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Be specific about objects and locations"}),"\n",(0,o.jsx)(n.li,{children:'Use spatial references when needed ("the cup on the left")'}),"\n",(0,o.jsx)(n.li,{children:"Break complex tasks into simpler commands if needed"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"error-recovery",children:"Error Recovery:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"If misunderstood, the system will ask for clarification"}),"\n",(0,o.jsx)(n.li,{children:"Commands can be modified or canceled during execution"}),"\n",(0,o.jsx)(n.li,{children:"Safety overrides always take precedence over voice commands"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"advanced-voice-plan-features",children:"Advanced Voice-PLAN Features"}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-processing",children:"Context-Aware Processing:"}),"\n",(0,o.jsx)(n.p,{children:'The system maintains conversation context to handle referential commands like "it" or "that one" based on previous interactions.'}),"\n",(0,o.jsx)(n.h3,{id:"multi-language-support",children:"Multi-Language Support:"}),"\n",(0,o.jsx)(n.p,{children:"Future implementations will support multiple languages with appropriate language models."}),"\n",(0,o.jsx)(n.h3,{id:"speaker-recognition",children:"Speaker Recognition:"}),"\n",(0,o.jsx)(n.p,{children:"Advanced systems can adapt to different users' speech patterns and preferences."}),"\n",(0,o.jsx)(n.p,{children:"These interactive examples demonstrate the sophisticated processing capabilities of the voice-PLAN system in the VLA architecture, showing how natural language commands are transformed into coordinated robotic actions while maintaining safety and contextual awareness."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},3023(e,n,i){i.d(n,{R:()=>s,x:()=>l});var t=i(6540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);