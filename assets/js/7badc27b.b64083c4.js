"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4371],{3023(e,n,t){t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},5671(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-4-vla/practical-demonstrations","title":"NAVIGATE and MANIPULATE Practical Demonstrations","description":"Overview","source":"@site/docs/module-4-vla/practical-demonstrations.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/practical-demonstrations","permalink":"/ai-native-book/docs/module-4-vla/practical-demonstrations","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/practical-demonstrations.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Voice-PLAN Interactive Examples","permalink":"/ai-native-book/docs/module-4-vla/voice-plan-examples"},"next":{"title":"Testing Complete VLA System Integration and Content","permalink":"/ai-native-book/docs/module-4-vla/system-integration-testing"}}');var i=t(4848),s=t(3023);const o={sidebar_position:9},r="NAVIGATE and MANIPULATE Practical Demonstrations",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Demonstration 1: Kitchen Assistance Scenario",id:"demonstration-1-kitchen-assistance-scenario",level:2},{value:"Scenario Description",id:"scenario-description",level:3},{value:"Step-by-Step Demonstration",id:"step-by-step-demonstration",level:3},{value:"Phase 1: Navigation (NAVIGATE System)",id:"phase-1-navigation-navigate-system",level:4},{value:"Phase 2: Object Identification (Vision System)",id:"phase-2-object-identification-vision-system",level:4},{value:"Phase 3: Grasp Planning (MANIPULATE System)",id:"phase-3-grasp-planning-manipulate-system",level:4},{value:"Phase 4: Manipulation Execution",id:"phase-4-manipulation-execution",level:4},{value:"Phase 5: Return Navigation",id:"phase-5-return-navigation",level:4},{value:"Demonstration 2: Cluttered Environment Navigation",id:"demonstration-2-cluttered-environment-navigation",level:2},{value:"Scenario Description",id:"scenario-description-1",level:3},{value:"Implementation Example",id:"implementation-example",level:3},{value:"Demonstration 3: Adaptive Manipulation Based on Object Properties",id:"demonstration-3-adaptive-manipulation-based-on-object-properties",level:2},{value:"Scenario Description",id:"scenario-description-2",level:3},{value:"Implementation Example",id:"implementation-example-1",level:3},{value:"Demonstration 4: Multi-Modal Integration Challenge",id:"demonstration-4-multi-modal-integration-challenge",level:2},{value:"Scenario Description",id:"scenario-description-3",level:3},{value:"Implementation Example",id:"implementation-example-2",level:3},{value:"Demonstration 5: Safety-First Implementation",id:"demonstration-5-safety-first-implementation",level:2},{value:"Scenario Description",id:"scenario-description-4",level:3},{value:"Implementation Example",id:"implementation-example-3",level:3},{value:"Practical Exercise: Implement Your Own Demonstration",id:"practical-exercise-implement-your-own-demonstration",level:2},{value:"Exercise Instructions:",id:"exercise-instructions",level:3},{value:"Example Scenario Framework:",id:"example-scenario-framework",level:3},{value:"Performance Metrics and Evaluation",id:"performance-metrics-and-evaluation",level:2},{value:"Navigation Metrics:",id:"navigation-metrics",level:3},{value:"Manipulation Metrics:",id:"manipulation-metrics",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"navigate-and-manipulate-practical-demonstrations",children:"NAVIGATE and MANIPULATE Practical Demonstrations"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This section provides practical demonstrations and implementation examples for the NAVIGATE and MANIPULATE systems in the Vision-Language-Action (VLA) architecture. These demonstrations illustrate real-world applications of autonomous navigation and manipulation capabilities in humanoid robots."}),"\n",(0,i.jsx)(n.h2,{id:"demonstration-1-kitchen-assistance-scenario",children:"Demonstration 1: Kitchen Assistance Scenario"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-description",children:"Scenario Description"}),"\n",(0,i.jsx)(n.p,{children:"The robot receives a command to navigate to the kitchen, identify a specific object (coffee mug), grasp it, and bring it to the user."}),"\n",(0,i.jsx)(n.h3,{id:"step-by-step-demonstration",children:"Step-by-Step Demonstration"}),"\n",(0,i.jsx)(n.h4,{id:"phase-1-navigation-navigate-system",children:"Phase 1: Navigation (NAVIGATE System)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Initialize navigation system\nnav_system = NavigationSystem()\n\n# Define target location\ntarget_location = "kitchen"\n\n# Plan path considering known obstacles\npath = nav_system.plan_path_to(target_location)\n\n# Execute navigation with safety monitoring\nfor waypoint in path:\n    # Check for dynamic obstacles\n    if nav_system.detect_dynamic_obstacles(waypoint):\n        nav_system.replan_path_to_avoid_obstacles()\n\n    # Move to waypoint\n    nav_system.move_to_waypoint(waypoint)\n\n    # Monitor progress\n    if not nav_system.verify_progress_towards(waypoint):\n        nav_system.handle_navigation_failure()\n\nprint("Successfully reached kitchen")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"phase-2-object-identification-vision-system",children:"Phase 2: Object Identification (Vision System)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Initialize vision system\nvision_system = VisionSystem()\n\n# Scan environment for target object\ntarget_object = {\n    "type": "coffee mug",\n    "color": "white",\n    "size": "medium"\n}\n\n# Detect and locate object\ndetected_objects = vision_system.scan_environment()\ntarget_mug = vision_system.identify_object(\n    objects=detected_objects,\n    criteria=target_object\n)\n\nif target_mug:\n    print(f"Found {target_mug[\'color\']} {target_mug[\'type\']} at {target_mug[\'position\']}")\nelse:\n    print("Target object not found")\n    # Implement search pattern\n    target_mug = vision_system.search_pattern(\n        area="kitchen_counter",\n        object_criteria=target_object\n    )\n'})}),"\n",(0,i.jsx)(n.h4,{id:"phase-3-grasp-planning-manipulate-system",children:"Phase 3: Grasp Planning (MANIPULATE System)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Initialize manipulation system\nmanip_system = ManipulationSystem()\n\nif target_mug:\n    # Plan optimal grasp based on object properties\n    grasp_plan = manip_system.plan_grasp(\n        object_info=target_mug,\n        robot_state=manip_system.get_robot_state()\n    )\n\n    # Verify grasp safety\n    safety_check = manip_system.validate_grasp_plan(grasp_plan)\n    if safety_check['is_safe']:\n        print(\"Grasp plan validated successfully\")\n    else:\n        print(f\"Grasp plan unsafe: {safety_check['violations']}\")\n        # Plan alternative grasp\n        grasp_plan = manip_system.plan_alternative_grasp(target_mug)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"phase-4-manipulation-execution",children:"Phase 4: Manipulation Execution"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Execute grasp with force control\ntry:\n    # Approach object\n    manip_system.move_to_approach_position(grasp_plan['approach_pose'])\n\n    # Execute grasp with force monitoring\n    grasp_success = manip_system.execute_grasp_with_force_control(\n        grasp_plan['grasp_pose'],\n        force_threshold=grasp_plan['force_threshold']\n    )\n\n    if grasp_success:\n        print(\"Successfully grasped the coffee mug\")\n\n        # Lift object safely\n        lift_pose = manip_system.compute_lift_pose(grasp_plan['grasp_pose'])\n        manip_system.move_to_pose(lift_pose)\n\n    else:\n        print(\"Grasp failed, attempting recovery\")\n        manip_system.handle_grasp_failure()\n\nexcept ManipulationError as e:\n    print(f\"Manipulation error: {e}\")\n    manip_system.emergency_stop()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"phase-5-return-navigation",children:"Phase 5: Return Navigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Plan path back to user\nuser_location = vision_system.locate_human_operator()\nreturn_path = nav_system.plan_path_to(user_location)\n\n# Execute return navigation while holding object\nfor waypoint in return_path:\n    # Special considerations when carrying object\n    nav_system.move_to_waypoint_with_careful_navigation(waypoint)\n\n    # Monitor object stability\n    if not manip_system.verify_object_stability():\n        nav_system.pause_for_stability_check()\n\nprint("Successfully delivered coffee mug to user")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"demonstration-2-cluttered-environment-navigation",children:"Demonstration 2: Cluttered Environment Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-description-1",children:"Scenario Description"}),"\n",(0,i.jsx)(n.p,{children:"Demonstrates the NAVIGATE system's ability to handle dynamic obstacles and replan in real-time in a cluttered environment."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-example",children:"Implementation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class DynamicNavigationDemo:\n    def __init__(self):\n        self.nav_system = NavigationSystem()\n        self.obstacle_detector = ObstacleDetectionSystem()\n        self.path_replanner = PathReplanningSystem()\n\n    def demonstrate_dynamic_navigation(self, target_location):\n        """Demonstrate navigation in environment with moving obstacles"""\n\n        # Initial path planning\n        current_path = self.nav_system.plan_path_to(target_location)\n        current_waypoint_idx = 0\n\n        while current_waypoint_idx < len(current_path):\n            current_waypoint = current_path[current_waypoint_idx]\n\n            # Check for obstacles ahead\n            obstacles_ahead = self.obstacle_detector.scan_ahead(\n                from_position=self.nav_system.get_current_position(),\n                to_position=current_waypoint\n            )\n\n            if obstacles_ahead:\n                print(f"Detected obstacles ahead: {obstacles_ahead}")\n\n                # Predict obstacle movement\n                predicted_paths = self.obstacle_detector.predict_obstacle_paths(\n                    obstacles_ahead,\n                    prediction_horizon=2.0\n                )\n\n                if self._paths_intersect_with_obstacles(\n                    current_path[current_waypoint_idx:],\n                    predicted_paths\n                ):\n                    print("Replanning to avoid moving obstacles...")\n\n                    # Replan path avoiding predicted obstacle locations\n                    new_path = self.path_replanner.replan_with_obstacle_prediction(\n                        start=self.nav_system.get_current_position(),\n                        goal=target_location,\n                        predicted_obstacles=predicted_paths\n                    )\n\n                    if new_path:\n                        current_path = new_path\n                        current_waypoint_idx = 0  # Start from beginning of new path\n                        continue\n                    else:\n                        # Wait for path to clear\n                        print("Waiting for path to clear...")\n                        self._wait_for_path_clearance(predicted_paths)\n\n            # Execute movement to current waypoint\n            self.nav_system.move_to_waypoint(current_waypoint)\n\n            # Verify progress and adjust if needed\n            if self._waypoint_reached(current_waypoint):\n                current_waypoint_idx += 1\n            else:\n                # Handle deviation from planned path\n                self._handle_navigation_deviation(current_waypoint)\n\n        print("Successfully reached target with dynamic obstacle avoidance")\n\n    def _paths_intersect_with_obstacles(self, path, predicted_obstacles):\n        """Check if path intersects with predicted obstacle locations"""\n        for point in path:\n            for obstacle_prediction in predicted_obstacles:\n                if self._point_near_obstacle_path(point, obstacle_prediction):\n                    return True\n        return False\n'})}),"\n",(0,i.jsx)(n.h2,{id:"demonstration-3-adaptive-manipulation-based-on-object-properties",children:"Demonstration 3: Adaptive Manipulation Based on Object Properties"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-description-2",children:"Scenario Description"}),"\n",(0,i.jsx)(n.p,{children:"Demonstrates how the MANIPULATE system adapts its approach based on different object characteristics."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-example-1",children:"Implementation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class AdaptiveManipulationDemo:\n    def __init__(self):\n        self.manip_system = ManipulationSystem()\n        self.object_classifier = ObjectClassificationSystem()\n\n    def demonstrate_adaptive_grasping(self, object_info):\n        \"\"\"Demonstrate adaptive manipulation based on object properties\"\"\"\n\n        # Classify object and adapt approach\n        object_category = self.object_classifier.categorize_object(object_info)\n\n        # Adjust parameters based on object category\n        manipulation_params = self._get_adaptive_parameters(object_category)\n\n        # Plan grasp with adaptive parameters\n        grasp_plan = self.manip_system.plan_grasp(\n            object_info=object_info,\n            parameters=manipulation_params\n        )\n\n        # Execute with appropriate force control\n        self._execute_adaptive_manipulation(\n            grasp_plan,\n            manipulation_params\n        )\n\n    def _get_adaptive_parameters(self, object_category):\n        \"\"\"Get manipulation parameters based on object category\"\"\"\n\n        adaptive_params = {\n            'fragile': {\n                'force_limit': 10.0,      # Low force for fragile items\n                'speed_factor': 0.3,      # Slow movement\n                'approach_distance': 0.05, # Gentle approach\n                'grasp_type': 'precision' # Precision grasp\n            },\n            'heavy': {\n                'force_limit': 80.0,      # Higher force for heavy objects\n                'speed_factor': 0.4,      # Moderate speed\n                'approach_distance': 0.15, # Standard approach\n                'grasp_type': 'power'     # Power grasp\n            },\n            'small': {\n                'force_limit': 15.0,      # Moderate force for small objects\n                'speed_factor': 0.6,      # Faster for small objects\n                'approach_distance': 0.08, # Precise approach\n                'grasp_type': 'precision' # Precision grasp\n            },\n            'large': {\n                'force_limit': 40.0,      # Standard force\n                'speed_factor': 0.5,      # Moderate speed\n                'approach_distance': 0.12, # Standard approach\n                'grasp_type': 'power'     # Power grasp for stability\n            },\n            'standard': {\n                'force_limit': 25.0,      # Default force\n                'speed_factor': 0.5,      # Default speed\n                'approach_distance': 0.10, # Default approach\n                'grasp_type': 'mixed'     # Mixed grasp strategy\n            }\n        }\n\n        return adaptive_params.get(object_category, adaptive_params['standard'])\n\n    def _execute_adaptive_manipulation(self, grasp_plan, params):\n        \"\"\"Execute manipulation with adaptive parameters\"\"\"\n\n        # Set force control parameters\n        self.manip_system.set_force_limit(params['force_limit'])\n\n        # Adjust movement speed\n        self.manip_system.set_movement_speed(params['speed_factor'])\n\n        # Execute grasp with adaptive approach\n        self.manip_system.execute_grasp_with_parameters(\n            grasp_plan=grasp_plan,\n            approach_distance=params['approach_distance'],\n            grasp_type=params['grasp_type']\n        )\n\n        # Verify success with category-appropriate criteria\n        success = self.manip_system.verify_grasp_success(\n            expected_object_category=params.get('object_category', 'standard')\n        )\n\n        return success\n"})}),"\n",(0,i.jsx)(n.h2,{id:"demonstration-4-multi-modal-integration-challenge",children:"Demonstration 4: Multi-Modal Integration Challenge"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-description-3",children:"Scenario Description"}),"\n",(0,i.jsx)(n.p,{children:"A complex task requiring both navigation and manipulation with real-time adaptation to environmental changes."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-example-2",children:"Implementation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MultiModalIntegrationDemo:\n    def __init__(self):\n        self.nav_system = NavigationSystem()\n        self.manip_system = ManipulationSystem()\n        self.vision_system = VisionSystem()\n        self.cognitive_planner = CognitivePlanner()\n        self.safety_monitor = SafetyMonitor()\n\n    def demonstrate_complex_task(self, user_command):\n        """Demonstrate complex task requiring multiple VLA components"""\n\n        print(f"Processing command: {user_command}")\n\n        # Step 1: Parse command with cognitive planner\n        task_decomposition = self.cognitive_planner.decompose_task(user_command)\n\n        # Step 2: Execute each subtask with monitoring\n        for i, subtask in enumerate(task_decomposition[\'subtasks\']):\n            print(f"Executing subtask {i+1}: {subtask[\'type\']}")\n\n            # Check safety before each subtask\n            if not self.safety_monitor.validate_subtask(subtask):\n                print(f"Subtask {i+1} failed safety check")\n                return self._handle_safety_violation(subtask)\n\n            try:\n                if subtask[\'type\'] == \'navigation\':\n                    self._execute_navigation_subtask(subtask)\n                elif subtask[\'type\'] == \'manipulation\':\n                    self._execute_manipulation_subtask(subtask)\n                elif subtask[\'type\'] == \'perception\':\n                    self._execute_perception_subtask(subtask)\n                else:\n                    raise ValueError(f"Unknown subtask type: {subtask[\'type\']}")\n\n                print(f"Subtask {i+1} completed successfully")\n\n            except Exception as e:\n                print(f"Subtask {i+1} failed: {e}")\n                recovery_result = self._attempt_recovery(subtask, e)\n\n                if not recovery_result[\'success\']:\n                    print(f"Recovery failed for subtask {i+1}")\n                    return self._handle_task_failure(task_decomposition, i)\n\n        print("Complex task completed successfully!")\n        return True\n\n    def _execute_navigation_subtask(self, subtask):\n        """Execute navigation component of subtask"""\n        target = subtask[\'target\']\n\n        # Update navigation plan with current environment\n        current_env = self.vision_system.scan_environment()\n        path = self.nav_system.plan_path_to(\n            target,\n            environment_map=current_env\n        )\n\n        # Execute navigation with dynamic obstacle detection\n        self.nav_system.execute_path_with_monitoring(\n            path=path,\n            safety_monitor=self.safety_monitor\n        )\n\n    def _execute_manipulation_subtask(self, subtask):\n        """Execute manipulation component of subtask"""\n        object_spec = subtask[\'object\']\n\n        # Locate object in current environment\n        target_object = self.vision_system.find_object_in_environment(\n            criteria=object_spec,\n            environment_map=self.vision_system.get_current_map()\n        )\n\n        if not target_object:\n            raise ManipulationError(f"Target object not found: {object_spec}")\n\n        # Plan and execute manipulation\n        grasp_plan = self.manip_system.plan_grasp(\n            object_info=target_object,\n            robot_state=self.manip_system.get_current_state()\n        )\n\n        self.manip_system.execute_grasp_with_monitoring(\n            grasp_plan=grasp_plan,\n            safety_monitor=self.safety_monitor\n        )\n\n    def _attempt_recovery(self, failed_subtask, error):\n        """Attempt to recover from subtask failure"""\n        recovery_strategies = {\n            \'navigation_failure\': self._recovery_navigation_failure,\n            \'manipulation_failure\': self._recovery_manipulation_failure,\n            \'perception_failure\': self._recovery_perception_failure\n        }\n\n        error_type = self._classify_error(error)\n        strategy = recovery_strategies.get(error_type, self._recovery_generic)\n\n        return strategy(failed_subtask, error)\n\n    def _recovery_navigation_failure(self, subtask, error):\n        """Recovery strategy for navigation failures"""\n        # Try alternative path\n        alternative_path = self.nav_system.find_alternative_path(\n            start=self.nav_system.get_current_position(),\n            goal=subtask[\'target\']\n        )\n\n        if alternative_path:\n            self.nav_system.execute_path(alternative_path)\n            return {\'success\': True, \'message\': \'Alternative path successful\'}\n\n        # If no alternative, report failure\n        return {\'success\': False, \'message\': \'No alternative path available\'}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"demonstration-5-safety-first-implementation",children:"Demonstration 5: Safety-First Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"scenario-description-4",children:"Scenario Description"}),"\n",(0,i.jsx)(n.p,{children:"Demonstrates safety protocols integrated throughout the NAVIGATE and MANIPULATE systems."}),"\n",(0,i.jsx)(n.h3,{id:"implementation-example-3",children:"Implementation Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SafetyFirstDemo:\n    def __init__(self):\n        self.nav_system = NavigationSystem()\n        self.manip_system = ManipulationSystem()\n        self.safety_system = SafetyMonitoringSystem()\n        self.emergency_stop = EmergencyStopSystem()\n\n    def demonstrate_safe_navigation(self, target):\n        """Demonstrate safe navigation with multiple safety layers"""\n\n        # Pre-execution safety checks\n        safety_check = self.safety_system.validate_navigation_plan(\n            target=target,\n            current_robot_state=self.nav_system.get_robot_state(),\n            environment=self.nav_system.get_environment_map()\n        )\n\n        if not safety_check[\'is_safe\']:\n            raise SafetyViolation(f"Navigation plan unsafe: {safety_check[\'violations\']}")\n\n        # Execute with continuous monitoring\n        path = self.nav_system.plan_path_to(target)\n\n        for waypoint in path:\n            # Check safety at each step\n            if not self.safety_system.is_safe_to_proceed(\n                current_position=self.nav_system.get_current_position(),\n                next_waypoint=waypoint\n            ):\n                self.emergency_stop.activate()\n                raise SafetyViolation("Unsafe condition detected during navigation")\n\n            # Execute movement\n            self.nav_system.move_to_waypoint(waypoint)\n\n            # Verify safety after movement\n            if not self.safety_system.verify_post_movement_safety():\n                self.emergency_stop.activate()\n                raise SafetyViolation("Safety violation after movement")\n\n    def demonstrate_safe_manipulation(self, target_object):\n        """Demonstrate safe manipulation with force control"""\n\n        # Pre-grasp safety validation\n        grasp_safety = self.safety_system.validate_grasp_plan(\n            object_info=target_object,\n            robot_state=self.manip_system.get_robot_state()\n        )\n\n        if not grasp_safety[\'is_safe\']:\n            raise SafetyViolation(f"Grasp unsafe: {grasp_safety[\'violations\']}")\n\n        # Execute with force monitoring\n        try:\n            # Approach with force limits\n            self.manip_system.move_to_approach_position(\n                target_object[\'pose\'],\n                force_limit=self.safety_system.get_approach_force_limit()\n            )\n\n            # Grasp with adaptive force control\n            grasp_result = self.manip_system.execute_grasp_with_force_control(\n                target_object[\'pose\'],\n                force_threshold=self.safety_system.get_grasp_force_threshold(target_object)\n            )\n\n            # Verify grasp safety\n            if not self.safety_system.verify_grasp_safety(grasp_result):\n                self.manip_system.release_object()\n                raise SafetyViolation("Grasp safety verification failed")\n\n        except ForceLimitExceeded:\n            self.emergency_stop.activate()\n            self.manip_system.emergency_release()\n            raise SafetyViolation("Force limit exceeded during manipulation")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-exercise-implement-your-own-demonstration",children:"Practical Exercise: Implement Your Own Demonstration"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-instructions",children:"Exercise Instructions:"}),"\n",(0,i.jsx)(n.p,{children:"Design and implement a practical demonstration that combines NAVIGATE and MANIPULATE capabilities. Your demonstration should:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Define a realistic scenario"})," involving both navigation and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement safety checks"})," throughout the execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handle potential failures"})," with appropriate recovery strategies"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adapt to environmental changes"})," during execution"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-scenario-framework",children:"Example Scenario Framework:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def my_vla_demonstration():\n    # Initialize systems\n    nav = NavigationSystem()\n    manip = ManipulationSystem()\n    vision = VisionSystem()\n    safety = SafetySystem()\n\n    # Define scenario parameters\n    scenario = {\n        'task': 'fetch_and_deliver',\n        'target_object': {'type': 'water_bottle', 'color': 'clear'},\n        'delivery_location': 'office_desk',\n        'safety_constraints': ['avoid_humans', 'preserve_object_integrity']\n    }\n\n    # Implement your demonstration logic here\n    # ...\n\n    return \"Demonstration completed successfully\"\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-metrics-and-evaluation",children:"Performance Metrics and Evaluation"}),"\n",(0,i.jsx)(n.h3,{id:"navigation-metrics",children:"Navigation Metrics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Path Efficiency"}),": Ratio of actual path length to optimal path length"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Success Rate"}),": Percentage of successful navigation attempts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Time to Goal"}),": Average time to reach target location"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety Incidents"}),": Number of safety system interventions"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"manipulation-metrics",children:"Manipulation Metrics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grasp Success Rate"}),": Percentage of successful grasps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Stability"}),": Percentage of objects maintained after grasp"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force Control Accuracy"}),": Precision of force application"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Completion Rate"}),": Percentage of successful task completions"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These practical demonstrations illustrate the sophisticated integration of NAVIGATE and MANIPULATE systems in the VLA architecture, showing how autonomous humanoid robots can safely and effectively perform complex tasks in real-world environments."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);